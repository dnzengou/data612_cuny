{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Research Discussion Assignment #2\n",
    "\n",
    "###### Zach Alexander - June 15th, 2020\n",
    "\n",
    "***\n",
    "\n",
    "### Instructions\n",
    "For this discussion item, please watch the talk and summarize what you found to be the most important or interesting points. The first half will cover some of the mathematical techniques covered in this unit's reading and the second half some of the data management challenges in an industrial-scale recommendation system.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Recommendations at Scale with Spark - Spotify Case Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christopher Johnson presented a talk at Spark Summit in 2014 that discussed Spotify's exploration of utilizing Apache Spark to scale their recommender system. As Christopher mentions towards the end of his talk, Spotify (at that point), hadn't found a way to completely scale their Spark system, and had been using Hadoop in production. I did a bit of side research out of general interest and found that Spotify is still using Spark for some of it's machine learning applications, but I'm not sure if they eventually fully transitioned over to utilizing Spark over Hadoop.\n",
    "\n",
    "***\n",
    "\n",
    "#### Building and scaling their recommender\n",
    "\n",
    "In the first part of the talk, Christopher goes into detail about some of the competitors of Spotify and how they've approached building a recommender engine for their products. Ultimately, it looks like Spotify's approach consists mainly of collaborative filtering, relying on user input, rather than some of the other popular approaches. Pandora had been utilizing a \"manual tag of attributes\" method, where they would hire song experts to tag each individual song (with hundreds of attributes), and then would build recommendations based on these tags. Songza and Beats, both of which had smaller catalogs, were relying on manual curation strategies. All of these methods seemed to have trouble scaling to Spotify's large user base and catalog of over 40 million songs. Therefore, they've gone all in on building an effective collaborative filtering recommender.\n",
    "\n",
    "***\n",
    "\n",
    "#### Explicit & Implicit Matrix Factorization\n",
    "\n",
    "After explaining the need to build out a recommender utilizing collaborative filtering, Christopher discusses how Spotify finds relationships between users and the songs they like to listen to. They've adopted a technique similar to Netflix's groundbreaking movie recommender (which operates on users **explicitly** recommending movies on a 1-5 scale), and have instead used a binary system of 0s and 1s to fill their matrices. The technique is through **implicit** matrix factorization, where a 1 is equal to a song that a user has listend to, and 0 is equal to a song that a user has not listened to. As you can imagine, these matrices will be quite sparse, where users will have a small subset of songs that will be equal to 1, however, there will be a huge number of songs that are equal to 0.\n",
    "\n",
    "Because of this, he breaks down methods of matrix decomposition, to reduce the number of dimensions of these user-song matrices. He first starts with the Netflix example with ratings between 1 and 5, but then shows the adapted approach utilizing Spotify's binary system. Essentially, the idea is to take the original ratings matrix, and reduce it to  much lower rank matrices (X and Y in his example), to something around rank=100. During this dimensionality reduction process, they can measure the loss in energy by calculating RMSE. For Spotify, since they are utilizing a binary system, they also need to weight their RMSE so that they can determine which songs are listened to by more users.\n",
    "\n",
    "***\n",
    "\n",
    "#### Alternating Least Squares (ALS)\n",
    "\n",
    "To solve, the team performs dimensionality reduction multiple times, weighting based on user input, and then solves by using Alternating Least Squares and weighted Ridge Regression until the values converge and they are able to get the optimal values.\n",
    "\n",
    "***\n",
    "\n",
    "#### Scaling this process in Hadoop and Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop\n",
    "\n",
    "Given the large catalog of songs and users, Spotify had to get creative with how they would build this out in Hadoop. Essentially, it appears that they \"blocked\" many of their processes, and grouped users based on ratings. This helped take care of a few things:\n",
    "\n",
    "+ Once users were separated into blocks, they only needed to grab the item vectors for that specific block -- this made processing time much faster\n",
    "\n",
    "+ After doing map/reduce, they could then solve for the optimal user vectors, with these already grouped efficiently. Again, cutting down on processing time.\n",
    "\n",
    "This had seemed to work for Spotify for a few years, and was the best solution they had come it up with at that point. However, there was a major issue with this, it created an IO bottleneck since they had to create more and more Hadoop jobs to process these tasks -- and they were reading and writing directly from disk, which was inefficient.\n",
    "\n",
    "##### Spark\n",
    "\n",
    "Fortunately, Spark has helped the team create a more efficient process. Instead of writing everything directly from disk, they can load the ratings matrix into memory, cache it, and then join items that are added.\n",
    "\n",
    "Christopher then went into detail about how they did the joins, attempting a few different methods:\n",
    "\n",
    "1) **Broadcast everything**: take the full ratings matrix, perform the Y transpose Y for dimensionality reduction, and then broadcast the item vectors to each worker, group the ratings by user, and then solve for the optimal user vector. The issue here was that they were shuffling data around on each iteration, and not caching the ratings data. They were also sending a full copy of the user/item vectors to all workers, which was inefficient.\n",
    "\n",
    "2) **Full gridify**: in a second attempt, they took the full ratings matrix and blocked it off into a grid of K by L partitions, cached them to each worker, and left them there (they didn't move). Compute Y transpose Y on the item vectors, identify which block/worker needed the item vectors, compute the intermediate terms, and then shuffle to group the items by user and solve/optimize. This was better since the ratings vector was cached and never shuffled, saving a lot in processing time, but the extra shuffling was not efficient on each iteration.\n",
    "\n",
    "3) **Half gridify**: the best solution at this point, they took the full ratings matrix and partitioned it into K by L partitions again, but also made sure to include all of the ratings per user in the same block before they cached them out to each worker. They then went through the same process of computing Y transpose Y, identified which block/worker needed the item vectors, but this time, they didn't need to shuffle since the user vectors were all in the same partition.\n",
    "\n",
    "As expected the \"half gridify\" method had the lowest running time, when compared to the \"full gridify\" method and their current Hadoop process in production. It shaved off about 8.5 hours of running time from their production system, when testing it across a subset of their 4 million users!\n",
    "\n",
    "***\n",
    "\n",
    "##### Spotify's Next Steps (as of 2014)\n",
    "\n",
    "After discussing some \"random learnings\" while he and the Spotify team got more familiar with Spark, he mentioned that they haven't been able to get the \"full gridify\" method scaled to production yet (as of 2014). He mentioned they were running into errors as they reached a ratings matrix of about 20M songs, so they still had Hadoop working in production, but were hoping to scale their Spark engine to their full catalog in the near future.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "+ https://www.youtube.com/watch?v=3LBgiFch4_g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
