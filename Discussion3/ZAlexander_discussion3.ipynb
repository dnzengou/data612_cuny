{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Research Discussion Assignment #3\n",
    "\n",
    "###### Zach Alexander - June 23th, 2020\n",
    "\n",
    "***\n",
    "\n",
    "### Instructions\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender Systems and Human Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In what ways do you think Recommender Systems reinforce human bias?\n",
    "\n",
    "After listening to Evan Estola discuss situations where recommender systems can go very wrong, and reading some of the other provided materials, I'm happy to see that data scientists are starting to take heed of these pitfalls and implementing ways to keep algorithms in check. In many ways, I think that Recommender Systems reinforce human bias, as by their very nature they operate to segment portions of a larger group based on commonalities. Some of these commonalities may be ethical and helpful, while others can be very detrimental to society. \n",
    "\n",
    "With this in mind, it's a data scientist's obligation to understand these processes and to mitigate human bias as much as possible. I came across this article in [Wired](https://www.wired.com/story/creating-ethical-recommendation-engines/), which discusses some very detrimental pitfalls of companies such as Facebook and Pinterest, and their lack of oversight and responsibility for their current use of recommender systems. \n",
    "\n",
    "**Facebook Groups**: Although the groups feature on Facebook has brought together people from all walks of life, and allowed society to build bridges across millions of different topics, this feature has been subject to a large degree of unethical activity. In many ways, Facebook has utilized groups to subset the information people receive, and has led to a \"global honeypot of spam, fake news, consipiracies, health misinformation, harrassment, hacking, trolling, scams, and other threats to users\". Through it's use of collaborative filtering, it's algorithms not only recommend groups to join based on user interests, but also subset a user's News Feed based on many of their affiliations. In many ways, this type of system can be very harmful, given that the news that is spread and digested operates in a vaccuum. Conspiracy theories, gamified news, and other forms of misinformation can spread virally based on user affiliations and large conspiracy-theory networks. The worst part about all of this is that Facebook continues to allow many aspects of this activity on its platform to go unchecked.  \n",
    "\n",
    "**Pinterest Boards**: Like many other social media sites, Pinterest boards have also been subject to bad use cases with their content-based and collaborative filtering techniques. As an example in the Wired article cited above, the author was served a foray of anti-Islamic memes and far-right videos that were perpetuated by Russian trolls after working on a disinformation project. When the author clicked on a few pins that were related to this content, suddenly they were served a large volume of these types of pins, replacing their other content of interest (wedding pins, naturescapes, etc.).\n",
    "\n",
    "In the end, I think recommender systems reinforce bias by their very nature. They are *supposed* to find content and fellow users that align with those clicking around on their site. When this happens, and when the algorithms aren't monitored or well understood, this leads to segmented groups of ideas, news, and knowledge.\n",
    "\n",
    "***\n",
    "\n",
    "##### Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  \n",
    "\n",
    "As mentioned above, content-based and collaborative filtering techniques are now being used everywhere (and in increasingly more advanced ways). As mentioned in Evan Estola's talk, Google implements numerous types of algorithms simultaneously on one search. I think that these systems do reinforce unethical targeting and customer segmentation, and I worry too that data scientists are implementing these algorithms without fully comprehending or understanding what they are doing. \n",
    "\n",
    "Additionally, it was interesting to see that two very similar Google searches can lead to *very* different website recommendations. I find this to be troubling, and with examples like these, it's difficult to see where this type of activity is helping rather than hurting our society.  \n",
    "\n",
    "I do think that people are typically going to search for content and join groups that they tend to truly believe, but for many, the internet is increasingly becoming the place where these recommender systems operate as a sort of societal haven, where certain ideas and news (sometimes misinformed and disturbing), can be reinforced and shared across broad platforms.  \n",
    "\n",
    "This is also true when it comes to targeting users. User-based and Item-based collaborative filtering will do a great job of subsetting and clustering users/items, but it will take a broader coalition of data scientists to intervene in ways to make sure that these groups are ethical and fair. Right now, much of my reading has shown me that this hasn't fully materialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "+ https://www.wired.com/story/creating-ethical-recommendation-engines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
