{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Research Discussion Assignment #4\n",
    "\n",
    "###### Zach Alexander - July 1st, 2020\n",
    "\n",
    "***\n",
    "\n",
    "### Instructions\n",
    "Read one or more of the articles below and consider how to counter the radicalizing effects of recommender systems or ways to prevent algorithmic discrimination.\n",
    "\n",
    "+ Renee Diresta, Wired.com (2018): Up Next: A Better Recommendation System\n",
    "\n",
    "+ Zeynep Tufekci, The New York Times (2018): YouTube, the Great Radicalizer\n",
    "\n",
    "+ Sanjay Krishnan, Jay Patel, Michael J. Franklin, Ken Goldberg (n/a): Social Influence Bias in Recommender Systems: A Methodology for Learning, Analyzing, and Mitigating Bias in Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countering Radicalizing Effects of Recommender Systems\n",
    "After reading Renee Diresta's article on *Wired* pertaining to ways that recommender systems are harming society, and mentioning possible solutions to address these pitfalls, it made me feel even more strongly that Tech companies that utilize this technology have a moral and ethical obligation to more strictly monitor the effects of these algorithms on its users. Although many find concern with this approach based on arguments centered around freedom of speech, we can visibly see the polarizing effect these systems have in the way society absorbs information, defines culture, and interacts with the offline world.\n",
    "\n",
    "##### Ways to counter polarizing effects\n",
    "As mentioned in the article, one way to reduce the potential that a recommender system may have an outsized radicalizing effect would be to set a defined threshold in a platform's content, and redirect users to less radical content if they surpass the previously defined threshold. This way, the system would subtly start to serve up less radical content, and keep users from continuing to progress down an undesired path of interaction and engagement. This approach would provide benefits in a few ways: 1) it would allow users to redirect radicalized viewpoints, that could potentially be harmful to a larger community, back towards more productive ones, and 2) as a side effect, this would disincentivize creators that are producing radical content, limiting their reach and exposure to the broader public. I do think projects like *Project Redirect* are a great start to combatting these flaws of recommender systems.\n",
    "\n",
    "Large social platforms that have outsized weight in the media market (i.e. Facebook, Twitter, YouTube, etc.), need to significantly increase their development around information security, content management, and information ethics. Given the impact that these platforms have in the way that society absorbs news and information, there needs to be more regulation around which sources and content are able to circulate to the broader public. If companies started to hire more people to investigate potentially harmful content, or content that is too radical for broad consumption, it would dissuade people from viewing and creating radical content.\n",
    "\n",
    "##### Ways to prevent algorithmic discrimination\n",
    "After watching last week's video by the lead Data Scientist at Meetup, I thought they had a great idea to limit discrimination in content they serve to their users. By splitting their recommender system into stages, where they isolate users by certain demographics, they build in bias checks to ensure that certain groups aren't unintentionally disenfranchised or discriminated against.\n",
    "\n",
    "I think it's easy for companies to prioritize the offline outputs of their recommender systems and the type of rankings they generate -- often times they lead to the most cost-effective and profit-driven conclusions, however, many times they don't factor in the effects they have when serving up recommendations to a broad, diverse audience. It would be useful for Data Science teams to draw conclusions from outputs of recommendations from the systems they create, but doing so with a critical eye.\n",
    "\n",
    "In the end, we have a moral responsibility to uphold certain values \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
