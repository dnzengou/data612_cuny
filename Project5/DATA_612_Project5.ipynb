{"cells":[{"cell_type":"markdown","source":["### DATA 612 - Project #5\n\nZach Alexander  \n7/4/2020"],"metadata":{}},{"cell_type":"markdown","source":["### Instructions\n\nAdapt one of your recommendation systems to work with Apache Spark and compare the performance with your previous iteration. Consider the efficiency of the system and the added complexity of using Spark. You may complete the assignment using PySpark (Python), SparkR (R), sparklyr (R), or Scala. Please include in your conclusion: For your given recommender systemâ€™s data, algorithm(s), and (envisioned) implementation, at what point would you see moving to a distributed platform such as Spark becoming necessary?"],"metadata":{}},{"cell_type":"markdown","source":["##### Declaring time-tracking functions to measure performance in PySpark \n\nBefore diving into the work, in order to measure and compare the performance of my work in Databricks (utilizing PySpark), relative to my work in R in previous weeks, I decided to load in two functions `tic()` and `toc()`. These will help set up a time window to measure the time elapsed from the beginning to the end of a certain piece of code. I'll use this throughout, but wanted to make sure I have these ready to go for my work in the following lines."],"metadata":{}},{"cell_type":"code","source":["def tic():\n    #Homemade version of matlab tic and toc functions\n    import time\n    global startTime_for_tictoc\n    startTime_for_tictoc = time.time()\n\ndef toc():\n    import time\n    if 'startTime_for_tictoc' in globals():\n        print(\"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\")\n    else:\n        print(\"Toc: start time not set\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["### Loading in Movies Data\n\nSimilar to how our professor walked through this process in our lecture this past week, I'll take a look at the MovieLens data, which I also did extensive analysis on the first few weeks of the course. Given that I ran multiple algorithms in past projects, and built recommender systems based on these ratings, I'll be able to see if an algorithm that I run on PySpark will perform better than my previous iterations in R."],"metadata":{}},{"cell_type":"markdown","source":["##### Saving the movies data as a csv file\n\nFirst, I saved the movies data as a separate csv file, and loaded it into the \"Data\" section of Databricks. After assigning it with a file type of .csv, indicating to read the first row as a header, and delimiter as a comma, I then created a Spark dataframe and saved it as `movies_df`. I then displayed this below:"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nmovies_file_location = \"/FileStore/tables/movies-2.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nmovies_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(movies_file_location)\n\n# display(movies_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Loading in the Ratings Data\n\nWith the movies data loaded in as a Spark dataframe, I then did the same process for the ratings data. Since we'll be utilizing the ratings data to build our model later, we also need to define a schema, indicating the data type for each of the four columns. I saved the schema as `movies_schema`, saved the ratings data in a dataframe, `ratings_df`, and displayed it below:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, IntegerType\n\nratings_file_location = \"/FileStore/tables/ratings-2.csv\"\n\nmovie_schema = StructType([\n  StructField(\"userId\", IntegerType()),\n  StructField(\"movieId\", IntegerType()),\n  StructField(\"rating\", DoubleType()),\n  StructField(\"timestamp\", DoubleType())\n])\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nratings_df = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .schema(movie_schema) \\\n  .load(ratings_file_location)\n\n# display(ratings_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["We can also take a quick look at the dimensions of the dataframe:"],"metadata":{}},{"cell_type":"code","source":["print(\"Number of rows in ratings_df dataset : {}\".format(str(ratings_df.count())))\nprint(\"Number of columns in ratings_df dataset : {}\".format(str(len(ratings_df.columns))))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of rows in ratings_df dataset : 1000209\nNumber of columns in ratings_df dataset : 4\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### Splitting into Training and Testing Datasets\n\nNow, with our ratings and movies data loaded into Databricks successfully, I then was able to split the full `ratings_df` into a training and test dataset. I decided to do a split of 80/20, similar to past weeks. In the end, we can see the dimensions of our training and test datasets below:"],"metadata":{}},{"cell_type":"code","source":["(training, test) = ratings_df.randomSplit([0.8, 0.2])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["print(\"Number of rows in training dataset : {}\".format(str(training.count())))\nprint(\"Number of columns in training dataset : {}\".format(str(len(training.columns))))\nprint(\"Number of rows in test dataset : {}\".format(str(test.count())))\nprint(\"Number of columns in test dataset : {}\".format(str(len(test.columns))))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of rows in training dataset : 800290\nNumber of columns in training dataset : 4\nNumber of rows in test dataset : 199919\nNumber of columns in test dataset : 4\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\ntic()\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\ntoc()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Elapsed time is 0.16474390029907227 seconds.\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["tic()\nmodel = als.fit(training)\ntoc()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Elapsed time is 15.110301971435547 seconds.\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["tic()\npredictions = model.transform(test)\ntoc()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Elapsed time is 0.02061295509338379 seconds.\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# display(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(\n    metricName=\"rmse\", labelCol=\"rating\", \n    predictionCol=\"prediction\")\n\nrmse = evaluator.evaluate(predictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Evaluation results\nprint(\"RMSE : {}\".format(str(rmse)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">RMSE : 0.8948988851599775\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### Assessing performance\n\nAfter running the matrix factorization method above (ALS), and measuring the elapsed time from fitting the training data to the model as well as the time it took to make the predictions, we can see the following when we compare these processing times to our SVD factorization method in R in past weeks:"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nperformance_table = pd.DataFrame([['PySpark', \"{:.2f}\".format(14.17) + \" seconds\", \"{:.2f}\".format(0.02) + \" seconds\"],\n             ['R', \"{:.2f}\".format(1.7) + \" seconds\", \"{:.2f}\".format(0.06) + \" seconds\"]],\n            columns = ['Platform', 'Fitting Training Data to Model/Computations', 'Making Predictions'])\n\nperformance_table"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Platform</th>\n      <th>Fitting Training Data to Model/Computations</th>\n      <th>Making Predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PySpark</td>\n      <td>14.17 seconds</td>\n      <td>0.02 seconds</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R</td>\n      <td>1.70 seconds</td>\n      <td>0.06 seconds</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["Interestingly, we can see that R seems to perform faster during the time it takes to fit the training data to the matrix factorization model. However, we see a noticeable difference in speed in making predictions between PySpark and R. PySpark was able to make predictions on the full ratings dataset about **3 times faster** than our predictions in R."],"metadata":{}},{"cell_type":"markdown","source":["### Second Algorithm Test -- Attempting to run SVD in PySpark and comparing to R\n\nAfter reading a fair bit of documentation, it seems like it's not very straightforward to run SVD in PySpark. However, I'm going to attempt to do it here and measure its performance against the process I outlined in my Project #3 assignment a few weeks ago. Although I've already seen that running a similar matrix factorization method (ALS) above is much faster in PySpark, I did want to see if I could do an \"apples to apples\" comparison from one platform to the other.\n\nTherefore, I initially decided to load in my user-movie matrix that I developed in R from Project #3. This matrix has already been imputed to have its missing values recalculated to the row mean. Therefore, this should be all set to run through SVD here in PySpark.\n\nBelow, you can see that I've successfully loaded in this user-movie matrix and assigned it to a spark dataframe of `R_movie_matrix`:"],"metadata":{}},{"cell_type":"code","source":["file_location = \"/FileStore/tables/movie_matrix-1.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nR_movie_matrix = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n# display(R_movie_matrix)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["We can confirm that this matrix has the same rows and columns as our matrix in R."],"metadata":{}},{"cell_type":"code","source":["print(\"Number of rows in R_movie_matrix dataset : {}\".format(str(R_movie_matrix.count())))\nprint(\"Number of columns in R_movie_matrix dataset : {}\".format(str(len(R_movie_matrix.columns))))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of rows in R_movie_matrix dataset : 943\nNumber of columns in R_movie_matrix dataset : 1664\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Next, we need to convert our spark dataframe into a RDD RowMatrix format in order to do matrix multiplication on it later. Therefore, I mapped through the dataframe and created a RowMatrix:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors, DenseMatrix\nfrom pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix, RowMatrix\n\nrdd = R_movie_matrix.rdd.map(list)\nmat = RowMatrix(rdd)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["With our RowMatrix ready to go, I then was able to use the `computeSVD()` function in PySpark to run singular value decomposition with our k value equal to 20, which was the same value as my optimal SVD computations in R in Project #3. The goal here is to obtain the same eigen values for s:"],"metadata":{}},{"cell_type":"code","source":["svd = mat.computeSVD(20, computeU=True)\nU = svd.U\ns = svd.s\nV = svd.V"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["After running SVD on our matrix, we can indeed see that we obtain the same eigenvalues for s:"],"metadata":{}},{"cell_type":"code","source":["print(\"Singular values are: %s\" % s)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Singular values are: [4528.795310209989,65.74346656442125,50.424327505714935,38.55470593522779,37.13876053402636,35.776545081960975,33.91172748271665,32.676419114502984,30.791885251212594,29.471643259235048,29.299125195958123,28.172323603732355,27.891424686553776,27.834347740275255,27.523725583591165,27.25689130512897,26.898323532598972,26.732033504109708,26.339615863951018,26.315940258488915]\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["I then was able to save my U, s, and V variables, and created a sigma variable below that has the eigenvalues of s oriented in a diagonal matrix. I was also able to compute the transpose of V. I created two new DenseMatrix variables in order to do this."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\nsigma = DenseMatrix(len(s), len(s), np.diag(s).ravel(\"F\"))\n\nV_transpose = DenseMatrix(V.numCols, V.numRows, V.toArray().transpose().ravel(\"F\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["With the new values computed and saved in proper PySpark formats, I was able to multiply the U orthonormal matrix by the sigma diagonal matrix by the V orthonormal matrix to obtain my SVD matrix with ratings predictions."],"metadata":{}},{"cell_type":"code","source":["mat_ = U.multiply(sigma).multiply(V_transpose)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["As we can see below, our new `mat_` matrix has the same dimensions as our original matrix we started with, which was to be expected. However, `mat_` should have our new prediction ratings computed for all user-movie combinations."],"metadata":{}},{"cell_type":"code","source":["print(mat_.numCols())\nprint(mat_.numRows())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1664\n943\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["After doing a quick check on my `movie_matrix_svd` matrix that I computed in R for Project #3, I'm able to calculate the same prediction ratings here in PySpark. We can confirm by looking at the values of the first user, which are indeed the same as the user-predicted ratings for my Project #3 dataframe in R."],"metadata":{}},{"cell_type":"code","source":["mat_.rows.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: DenseVector([4.3125, 3.6207, 3.3733, 3.5236, 3.2783, 3.7232, 4.6954, 3.9236, 4.4911, 3.7586, 3.3358, 4.4699, 4.2959, 4.1518, 3.7446, 3.7951, 3.2872, 3.6105, 3.6634, 3.6418, 3.089, 3.6511, 3.6332, 3.8188, 4.0597, 3.4801, 3.4696, 3.9281, 2.7461, 3.648, 3.7341, 3.7167, 3.5756, 3.4375, 3.2689, 3.52, 3.4125, 3.2084, 3.5867, 3.5844, 3.3074, 3.8738, 3.5558, 3.5316, 3.9259, 3.7229, 3.3587, 4.2922, 3.5675, 6.1129, 3.6564, 3.8527, 3.5882, 3.4971, 3.6998, 4.6262, 3.7444, 4.1034, 4.0697, 4.0725, 3.8666, 3.8587, 3.2086, 4.0968, 3.696, 3.5315, 2.9933, 3.5075, 3.1521, 3.7775, 3.4479, 3.6162, 3.4477, 3.3748, 3.5951, 3.517, 3.5605, 3.0076, 3.907, 3.4685, 3.6029, 3.8139, 4.1928, 3.6329, 3.3725, 3.6628, 4.0963, 3.2808, 4.9756, 3.5387, 3.9126, 3.7453, 3.7994, 2.9252, 3.575, 4.6667, 3.4569, 4.2363, 3.2097, 5.7657, 3.4922, 3.4315, 3.1877, 3.3137, 3.2458, 3.5854, 3.7601, 3.6746, 4.1478, 3.3467, 4.0255, 3.2846, 3.7163, 4.1739, 3.6946, 3.6818, 3.5937, 2.9805, 3.7315, 3.1, 3.8342, 3.1284, 3.5344, 4.4217, 3.3201, 3.7643, 4.2876, 3.7392, 4.0137, 3.429, 3.2623, 3.53, 3.5769, 4.2008, 4.1415, 3.6845, 4.4645, 3.1266, 3.4084, 2.992, 3.4659, 3.3409, 2.6779, 4.1951, 3.1969, 3.4828, 3.3957, 3.13, 3.5639, 4.0944, 3.5643, 4.0732, 4.3921, 4.217, 2.9404, 3.7708, 3.8651, 3.123, 3.2351, 3.5779, 3.5749, 3.283, 3.819, 3.9179, 3.8491, 3.9333, 3.3576, 4.7677, 4.9377, 4.1097, 4.016, 5.2659, 5.4315, 5.4305, 4.1141, 4.593, 3.9236, 4.0654, 3.6679, 3.3968, 5.3663, 3.7418, 4.7849, 4.082, 3.6762, 4.2253, 3.6744, 3.6632, 4.1705, 4.0956, 4.1477, 3.4258, 3.73, 4.2364, 4.3623, 4.2516, 3.7301, 4.0261, 4.1747, 3.3487, 3.7251, 4.1982, 3.529, 4.6041, 3.4519, 3.6774, 3.8928, 4.3369, 3.9823, 4.437, 4.2953, 3.9948, 3.7012, 3.5395, 3.2427, 3.9432, 3.391, 3.192, 2.995, 3.3283, 3.8216, 4.0011, 4.4183, 3.8823, 2.673, 3.546, 3.6157, 4.7082, 3.6188, 3.9307, 2.6769, 3.3136, 3.4274, 3.6152, 3.1831, 3.7375, 3.4899, 3.9955, 3.8702, 2.7947, 3.5737, 3.6491, 2.6623, 3.3663, 2.9253, 4.0781, 3.2919, 4.077, 3.4492, 3.7495, 3.8465, 3.3167, 3.529, 3.2142, 3.3421, 3.6234, 4.1467, 5.3402, 3.2147, 3.1874, 2.9448, 3.7683, 3.3109, 3.3537, 4.1552, 3.1743, 3.7107, 3.8728, 3.7375, 3.474, 3.8866, 3.6048, 3.5619, 4.2419, 5.075, 3.6372, 3.2696, 3.6436, 3.4732, 3.2781, 3.4101, 3.7753, 3.1669, 4.1351, 2.9777, 3.4585, 3.6188, 3.2273, 3.6352, 3.3449, 3.5049, 3.7491, 3.691, 3.6075, 3.5948, 3.7677, 3.78, 3.3624, 3.4206, 3.4971, 4.1123, 3.7372, 3.633, 3.6017, 3.4674, 3.6639, 3.5286, 3.6614, 3.6496, 3.3065, 3.7273, 3.4643, 3.6951, 3.6272, 3.8448, 3.9568, 3.7065, 3.5333, 3.5507, 2.9916, 3.6175, 3.8484, 3.4864, 3.509, 3.6899, 3.2892, 3.5418, 3.5128, 3.5484, 3.4069, 3.6297, 3.4227, 3.642, 3.4028, 3.5732, 3.0091, 3.5178, 3.4602, 3.5727, 3.589, 3.3908, 3.4978, 3.7523, 3.6485, 3.6564, 3.4546, 3.5669, 3.5279, 3.6689, 3.6528, 3.5763, 3.6029, 3.2623, 3.537, 3.2566, 3.5818, 3.6362, 3.5839, 3.6588, 3.6985, 3.4435, 3.539, 3.4252, 3.6553, 3.0645, 3.153, 3.3469, 3.5094, 3.7285, 3.5252, 3.4642, 3.2538, 3.3624, 3.3669, 3.1457, 3.2352, 3.6889, 3.6258, 3.8448, 3.5028, 3.3707, 3.6899, 3.2387, 3.4635, 3.2268, 3.4849, 3.7329, 3.565, 3.4074, 2.956, 3.5247, 3.2183, 3.3994, 3.462, 3.5158, 3.7687, 3.7139, 3.5166, 3.1758, 3.5507, 3.2424, 3.3472, 3.4191, 3.5258, 4.5797, 3.2505, 2.888, 3.1694, 2.9463, 3.5363, 3.7012, 3.5513, 3.4004, 3.2211, 2.9795, 2.9936, 3.3553, 3.8487, 3.5587, 3.683, 3.1655, 3.7751, 3.4701, 3.6108, 3.9936, 3.8414, 4.1504, 3.4804, 3.4529, 4.1613, 3.883, 4.0351, 3.9335, 3.4412, 3.4378, 3.4412, 3.3501, 3.2369, 3.4704, 3.7231, 3.4497, 3.5705, 3.6114, 3.4941, 3.2683, 3.6972, 3.3386, 3.077, 3.0074, 3.28, 3.5562, 4.1557, 3.0522, 3.0093, 3.4758, 3.5894, 3.6861, 3.6014, 3.984, 3.7573, 3.7423, 3.43, 3.7794, 3.6943, 3.4246, 3.8053, 3.4341, 3.5539, 2.8028, 3.2901, 4.1694, 3.8976, 3.0209, 3.7297, 3.8422, 4.1077, 3.7106, 3.5606, 3.7452, 4.266, 4.0319, 3.2484, 3.6876, 3.6624, 3.5527, 3.7038, 3.6166, 3.8278, 3.6836, 3.7845, 3.6635, 3.5206, 3.7215, 3.72, 3.9188, 3.6557, 3.1479, 3.8968, 3.6246, 3.6866, 3.6615, 3.6761, 3.4819, 4.1714, 3.9935, 3.9729, 4.224, 3.8852, 3.7162, 4.0151, 3.855, 3.8725, 3.6592, 3.7298, 3.7708, 3.8864, 3.7388, 3.8027, 3.8473, 3.702, 3.7365, 3.7678, 3.8516, 3.7566, 3.8779, 3.8888, 3.7095, 3.5258, 3.6012, 3.5348, 3.4769, 3.6334, 3.6837, 3.4992, 3.367, 3.595, 3.4484, 3.3047, 3.7102, 3.3836, 3.6584, 3.1122, 3.426, 3.5898, 3.7257, 3.7206, 3.5379, 3.5449, 3.528, 3.1725, 3.5979, 3.6577, 3.602, 3.755, 2.9739, 3.6365, 3.3728, 3.4741, 3.5791, 3.3419, 3.4734, 3.6321, 3.4827, 3.6606, 3.1299, 3.5895, 3.1424, 3.4697, 3.4889, 3.61, 3.1481, 3.5962, 3.0805, 3.6345, 3.5838, 3.6347, 3.5164, 3.4737, 3.7511, 3.4215, 3.6103, 3.5159, 3.5946, 3.5245, 3.6072, 3.6558, 3.6815, 3.6308, 3.6614, 3.5201, 3.6681, 3.4178, 3.0077, 3.5805, 3.6336, 3.5889, 3.6426, 3.5477, 3.9048, 3.6353, 3.5595, 3.6544, 3.642, 3.6458, 3.4569, 3.5602, 3.6433, 3.6017, 3.7326, 3.7478, 3.7569, 3.5669, 3.5955, 3.5892, 3.4764, 3.5369, 3.6683, 3.6406, 3.4982, 3.5653, 3.4101, 3.6423, 3.5995, 3.5514, 3.474, 3.6066, 3.4487, 3.6354, 3.5625, 3.5452, 3.3781, 3.7246, 3.4934, 3.6844, 3.6718, 3.5722, 3.6811, 3.6829, 3.6543, 3.4007, 3.7025, 3.6268, 3.8914, 3.719, 3.6307, 3.4368, 3.8256, 3.9922, 3.6199, 3.7755, 3.7705, 3.7619, 3.8607, 3.4608, 3.5656, 4.0334, 3.6715, 3.6279, 3.6291, 3.6837, 2.9795, 3.6028, 3.5912, 3.5417, 3.6182, 3.4344, 3.3295, 3.244, 3.5595, 3.7688, 3.5349, 3.6144, 3.1538, 3.5701, 3.4221, 3.4498, 3.25, 3.4727, 3.6902, 3.762, 3.1283, 3.1841, 3.2732, 3.4368, 3.6385, 3.4017, 3.6459, 3.8374, 3.6316, 3.5573, 3.6396, 3.6202, 3.7966, 3.7081, 3.6278, 3.6375, 3.6228, 3.5537, 3.8071, 3.4847, 3.6113, 3.8595, 4.1307, 3.5926, 3.6067, 3.7935, 3.5906, 3.6237, 3.8232, 3.5104, 3.3359, 3.6817, 3.5878, 3.4561, 3.5811, 3.5863, 3.6855, 3.8777, 3.705, 3.5733, 3.6004, 3.4609, 3.4964, 3.684, 3.5635, 3.5595, 3.7453, 3.4981, 3.912, 3.5351, 3.5846, 3.4876, 3.1692, 3.6065, 3.6317, 3.3836, 3.5313, 3.5049, 3.6663, 4.0354, 3.8217, 3.286, 3.4556, 3.8153, 3.6364, 3.4933, 3.5873, 3.6725, 3.5167, 3.2721, 3.6458, 3.2986, 3.6506, 3.192, 3.668, 3.7548, 3.0704, 3.4781, 3.6479, 3.6656, 3.5375, 3.242, 3.3465, 3.9076, 3.6702, 3.6915, 3.579, 3.4353, 3.569, 3.6227, 3.544, 3.6982, 3.6331, 3.0331, 3.3106, 3.5568, 3.4432, 3.5557, 3.5534, 3.5879, 3.5486, 3.5558, 3.7892, 3.3723, 3.6034, 3.5511, 3.7582, 3.4146, 3.3994, 3.5544, 3.5931, 3.6159, 3.6301, 3.4199, 3.563, 3.5122, 3.5388, 3.5883, 3.6933, 3.8165, 3.5517, 3.5749, 3.5832, 3.5593, 3.7003, 3.4958, 3.6043, 3.694, 3.4004, 3.2104, 3.5087, 3.5751, 3.5239, 3.2395, 3.6379, 3.5877, 3.4793, 3.6234, 3.1549, 3.4409, 3.438, 3.495, 3.4539, 3.5072, 3.375, 3.6656, 3.5254, 3.3252, 3.5926, 3.5677, 3.6252, 3.5378, 3.4871, 3.38, 3.4125, 3.3583, 3.5606, 3.5252, 3.4507, 3.4814, 3.8137, 3.6735, 3.3882, 3.6593, 3.7398, 3.5072, 3.596, 3.5281, 3.8192, 3.8213, 3.6006, 3.5325, 3.5618, 3.5625, 3.6505, 3.4081, 3.5773, 3.547, 3.4259, 3.7174, 3.6764, 3.4433, 3.5988, 3.3545, 3.6591, 3.489, 3.6162, 3.3948, 3.3461, 3.4525, 3.3348, 3.3702, 3.7034, 3.5437, 3.5428, 3.5025, 3.3921, 3.7186, 3.7576, 3.5093, 3.594, 3.105, 3.5166, 3.4417, 3.4939, 3.5068, 3.5019, 3.8069, 3.5052, 3.4711, 3.4907, 3.7041, 3.6041, 3.5858, 3.6114, 3.4993, 3.4703, 3.5569, 3.5032, 3.5413, 3.6628, 3.548, 3.5515, 3.5707, 3.5103, 3.5326, 3.7609, 3.5933, 3.6687, 3.618, 3.9435, 3.6448, 3.7374, 3.5637, 3.598, 3.4134, 3.505, 3.1536, 3.6034, 3.4327, 3.4623, 3.375, 2.781, 3.5102, 3.5742, 3.4156, 3.6254, 3.686, 3.6113, 3.6551, 3.3425, 3.7392, 3.5515, 3.7459, 3.6135, 3.5451, 3.7397, 3.2153, 3.63, 3.2971, 3.325, 3.5493, 3.3871, 3.6356, 3.5871, 3.4593, 3.626, 3.6157, 3.6432, 3.6567, 3.52, 3.6013, 3.5646, 3.6385, 3.8566, 3.6682, 3.6017, 3.6382, 3.6119, 3.625, 3.3791, 3.6554, 3.7074, 3.5407, 3.6015, 3.5803, 3.4246, 3.5126, 3.4321, 3.3426, 3.5274, 3.6419, 3.5171, 3.7031, 3.5985, 3.3706, 3.5983, 3.3739, 3.6163, 3.2758, 3.5472, 3.3836, 3.6474, 3.6364, 3.6204, 3.6463, 3.5903, 3.5005, 3.6204, 3.4683, 3.6368, 3.5785, 3.3328, 3.5438, 3.6441, 3.781, 3.5688, 3.8492, 3.6076, 3.6314, 3.7073, 3.5481, 3.8395, 3.5046, 3.301, 3.5769, 3.5422, 3.5174, 3.6652, 3.6379, 3.7236, 3.6398, 3.6491, 3.363, 3.601, 3.2905, 3.5813, 3.6861, 3.0538, 3.6427, 3.5378, 3.5532, 3.605, 3.3506, 3.6362, 3.3971, 3.5749, 3.1221, 3.5366, 3.6526, 3.7253, 3.1805, 3.5997, 3.6615, 3.4551, 3.573, 3.4989, 2.9527, 3.3044, 3.4964, 3.5098, 3.6166, 3.5648, 3.3592, 3.5846, 3.5432, 3.6525, 3.8349, 3.6655, 3.4106, 3.2788, 3.4996, 3.6454, 3.3814, 3.598, 3.5273, 3.5661, 3.7546, 3.5959, 3.5881, 3.6941, 3.6436, 3.6395, 3.8156, 3.2946, 3.5932, 3.515, 3.5089, 3.5899, 3.3078, 3.5853, 3.5869, 3.4787, 3.6466, 3.6233, 3.7127, 3.6657, 3.5624, 3.728, 3.4716, 3.6, 3.4572, 3.4969, 3.6378, 3.6152, 3.4112, 3.5907, 3.5891, 3.6261, 3.6086, 3.6364, 3.6664, 3.6139, 3.6232, 3.6368, 3.506, 3.6026, 3.5761, 3.5192, 3.5957, 3.659, 3.7117, 3.494, 3.6549, 3.6303, 3.5695, 3.5721, 3.5707, 3.5612, 3.6101, 3.4681, 3.6274, 3.613, 3.6044, 3.5964, 3.6006, 3.5419, 3.6209, 3.54, 3.6781, 3.6205, 3.6425, 3.329, 3.5399, 3.5997, 3.3828, 3.4869, 3.6647, 3.5932, 3.3483, 3.443, 3.6423, 3.7344, 3.6852, 3.6151, 3.5781, 3.5645, 3.6111, 3.6496, 3.6231, 3.5719, 3.4864, 3.5669, 3.6116, 3.4754, 3.6057, 3.6047, 3.4833, 3.603, 3.6408, 3.642, 3.5919, 3.5555, 3.558, 3.5756, 3.599, 3.6297, 3.7097, 3.6349, 3.6086, 3.4711, 3.6065, 3.5232, 3.5861, 3.6099, 3.6447, 3.3895, 3.5627, 3.6547, 3.5859, 3.5802, 3.6324, 3.7054, 3.4804, 3.7198, 3.3957, 3.5961, 3.5478, 3.389, 3.6477, 3.6057, 3.6021, 3.6425, 3.6659, 3.7498, 3.5582, 3.6069, 3.6454, 3.5845, 3.6699, 3.6525, 3.6082, 3.6405, 3.6269, 3.5909, 3.5984, 3.6227, 3.6729, 3.6016, 3.5188, 3.6374, 3.5907, 3.6593, 3.56, 3.456, 3.4186, 3.6653, 3.5555, 3.2989, 3.6179, 3.4618, 3.739, 3.6452, 3.5969, 3.6288, 3.6695, 3.6218, 3.4453, 3.509, 3.5347, 3.5534, 3.5703, 3.6545, 3.5968, 3.6025, 3.6035, 3.6053, 3.6076, 3.6039, 3.5251, 3.726, 3.5668, 3.4659, 3.6036, 3.536, 3.5227, 3.6769, 3.5556, 3.6438, 3.6428, 3.4215, 3.6071, 3.6087, 3.6779, 3.5508, 3.6312, 3.6183, 3.393, 3.6051, 3.6437, 3.6557, 3.6397, 3.6399, 3.6093, 3.4467, 3.522, 3.6322, 3.6534, 3.6361, 3.6165, 3.659, 3.5766, 3.5346, 3.7214, 3.6276, 3.4615, 3.6208, 3.6094, 3.6606, 3.5666, 3.5529, 3.6292, 3.7145, 3.6004, 3.5524, 3.6163, 3.606, 3.6541, 3.688, 3.648, 3.6986, 3.618, 3.5994, 3.5641, 3.5502, 3.544, 3.6109, 3.5744, 3.6024, 3.5693, 3.6167, 3.5857, 3.5632, 3.6908, 3.663, 3.6512, 3.6555, 3.657, 3.6085, 3.6056, 3.5931, 3.6037, 3.6151, 3.6396, 3.5086, 3.6163, 3.6441, 3.6339, 3.4726, 3.5995, 3.6343, 3.5789, 3.599, 3.5695, 3.5995, 3.6191, 3.5949, 3.5711, 3.5995, 3.6026, 3.6017, 3.5931, 3.5925, 3.648, 3.5948, 3.3068, 3.5342, 3.6494, 3.5995, 3.5995, 3.5995, 3.5995, 3.5995, 3.5978, 3.5955, 3.6447, 3.5936, 3.5995, 3.5995, 3.6021, 3.5845, 3.5995, 3.6482, 3.5921, 3.5947, 3.6076, 3.6089, 3.5905, 3.648, 3.5777, 3.5762, 3.5993, 3.5995, 3.5995, 3.5987, 3.5995, 3.6267, 3.6095, 3.6, 3.6002, 3.5949, 3.6014, 3.5995, 3.6026, 3.5665, 3.6592, 3.5721, 3.6014, 3.5784, 3.5952, 3.6098, 3.648, 3.5993, 3.6477, 3.6217, 3.5916, 3.5898, 3.596, 3.5964, 3.5992, 3.6463, 3.5497, 3.5627, 3.6156, 3.5834, 3.6033, 3.5809, 3.6211, 3.6785, 3.6733, 3.6067, 3.5111, 3.6238, 3.6298, 3.7077, 3.5878, 3.5723, 3.6015, 3.6393, 3.6255, 3.5203, 3.5744, 3.6733, 3.6116, 3.6555, 3.63, 3.6076, 3.6005, 3.4614, 3.5481, 3.6817, 3.6503, 3.6347, 3.6646, 3.555, 3.6506, 3.6033, 3.5269, 3.6526, 3.5983, 3.6074, 3.6443, 3.6026, 3.6499, 3.6351, 3.603, 3.57, 3.6682, 3.6354, 3.6107, 3.6127, 3.6463, 3.5657, 3.5148, 3.6551, 3.5354, 3.6053, 3.6231, 3.6665, 3.6079, 3.6251, 3.6037, 3.6069, 3.5928, 3.6035, 3.6029, 3.6053, 3.6037, 3.6216, 3.6053, 3.6069, 3.6207, 3.6099, 3.6448, 3.6022, 3.5987, 3.6252, 3.6615, 3.5897, 3.5744, 3.5955, 3.603, 3.6051, 3.6428, 3.637, 3.5937, 3.5934, 3.5088, 3.6781, 3.5164, 3.6337, 3.6073, 3.6289, 3.6412, 3.6628, 3.4975, 3.538, 3.6817, 3.5425, 3.6433, 3.6666, 3.6399, 3.4975, 3.4975, 3.6734, 3.5831, 3.5451, 3.6399, 3.6856, 3.6916, 3.4877, 3.6554, 3.6596, 3.589, 3.6095, 3.6148, 3.5957, 3.6333, 3.6132, 3.6086, 3.6108, 3.5998, 3.5814, 3.6538, 3.608, 3.6438, 3.5835, 3.5914, 3.5851, 3.6268, 3.5855, 3.6312, 3.5876, 3.622, 3.6144, 3.6051, 3.6, 3.6053, 3.6752, 3.6567, 3.6334, 3.5975, 3.6056, 3.6216, 3.6529, 3.602, 3.6143, 3.6002, 3.6772, 3.5489, 3.6103, 3.608, 3.6049, 3.6524, 3.5773, 3.6536, 3.5922, 3.6536, 3.651, 3.5419, 3.6563, 3.6245, 3.641, 3.4772, 3.6437, 3.6574, 3.6536, 3.689, 3.6536, 3.6624, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6538, 3.6536, 3.6536, 3.6536, 3.6536, 3.6538, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6536, 3.6567, 3.6536, 3.6536, 3.6533, 3.6701, 3.6533, 3.6354, 3.6706, 3.6105, 3.6416, 3.5963, 3.5963, 3.5354, 3.6274, 3.6198, 3.6073, 3.6026, 3.6066, 3.611, 3.6017, 3.5983, 3.5847, 3.4774, 3.5803, 3.5997, 3.6191, 3.6054, 3.6048, 3.5796, 3.6161, 3.5805, 3.6045, 3.4671, 3.6917, 3.5956, 3.6164, 3.6022, 3.5003, 3.603, 3.6021, 3.623, 3.6054, 3.6107, 3.6054, 3.6054, 3.6025, 3.6054, 3.6082, 3.6054, 3.6054, 3.6083, 3.6054, 3.6054, 3.6123, 3.6088, 3.6008, 3.6082, 3.6034, 3.6054, 3.6025, 3.6054, 3.6082, 3.6028, 3.5992, 3.6052, 3.624, 3.6047, 3.6123, 3.61, 3.5906, 3.6035, 3.6062, 3.6079, 3.6062, 3.6062, 3.6048, 3.6048, 3.6062, 3.6048, 3.6099, 3.5923, 3.6116, 3.6058, 3.6061, 3.6082, 3.6025, 3.6067, 3.6052, 3.6054, 3.5972])</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["### Assessing performance of SVD in PySpark and R\n\nAfter doing this apples-to-apples comparison between the two platforms, we can see the following processing time breakdown:"],"metadata":{}},{"cell_type":"code","source":["performance_table = pd.DataFrame([['PySpark SVD', \"{:.2f}\".format(502) + \" seconds\", \"{:.2f}\".format(0.07) + \" seconds\"],\n             ['R SVD', \"{:.2f}\".format(1.7) + \" seconds\", \"{:.2f}\".format(0.06) + \" seconds\"]],\n            columns = ['Platform/Method', 'Computing SVD', 'Making Predictions'])\n\nperformance_table"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Platform/Method</th>\n      <th>Computing SVD</th>\n      <th>Making Predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PySpark SVD</td>\n      <td>502.00 seconds</td>\n      <td>0.07 seconds</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R SVD</td>\n      <td>1.70 seconds</td>\n      <td>0.06 seconds</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["### Conclusion\n\nAfter working through both ALS and SVD in PySpark, I've found a few takeaways:  \n\n+ ALS in PySpark seemed to perform about **three times faster** than SVD in R when looking strictly at its ability to make ratings predictions  \n+ ALS in PySpark was much slower than R, about 14 seconds to 2 seconds, in terms of fitting the training data to the model and computing singular values in R (although this is not a true comparison, so doesn't hold much weight)  \n\nAlthough we cannot really draw too many concrete conclusions from this initial comparison, it does look like PySpark may be more efficient at making predictions once the data has been processed and/or fit to the training data. When we ran the SVD technique in PySpark as well, and compared this to the same computations from Project #3, this seems to confirm these findings:  \n\n+ SVD computation in PySpark was *very* slow, taking about 502 seconds (~8.5 minutes) with a k-value of 20. This was much slower than the approximately 2 seconds it took to run this function in R.  \n+ However, once SVD had been computed in PySpark, it seemed to make predictions on our user-movie matrix **at about the same efficiency** as we saw in R (about 0.06 seconds for each).  \n\n\n##### Envisioned Implimentation\nThis makes me consider a few things about implimentation -- 1) Using SVD in PySpark may not be the most efficient technique to use for matrix factorization and dimensionality reduction. If we were to role this out to production, we'd seriously want to think through the implications of using a technique that takes about 8.5 minutes to process a relatively small dataset of ratings. If SVD did yield the best predictions and was chosen to role out to production, it would be important to think about how this system would handle inputs of new ratings. Likely, it would be advantageous to run updates to the predictions (based on new data) in batches, in order to prevent slow processing times on the front-end of an application. However, if it were possible to use ALS as a good alternative to SVD, it may be more worthwhile to do so. 2) PySpark seems to do a good job of making predictions and utilizing the data it has available once it has been processed. Although we saw slower processing times in PySpark than we did in R, the efficiencies of making faster predictions may be more beneficial when thinking about implementation. As mentioned above, batch updates could help keep the models up to date as much as possible, while still having the advantages of faster processing time for predictions. In the end, it would be important to consider the pros and cons to these two factors in determining which algorithms, techniques and platforms to use to implement a recommender system in production.  \n\n##### Transitioning to Distributed Platform\nI could definitely see it becoming necessary to transition to a distributed platform when the volume of data reaches into the millions and billions. Given that PySpark would be able to handle computations in a distributed fashion, eventually the larger your data volume, the more efficient it'll be. For our example utilizing the MovieLens data (and doing computations over a thousand rows), we didn't see the full advantages of Spark. However, once an application or system is scaled to meet the demand of a much larger volume of users/clients (i.e. millions/billions, big data, etc.), it would not be possible to process these consuming algorithms and techniques in a non-distributed fashion.\n\n##### Added Complexity of Using Spark\nI will say that it was a bit tricky to get acclimated to Spark this week. The resources are helpful, but at this point in time, it's difficult for me to fully wrap my head around the nuances of distributed data types such as RowMatrices, Spark dataframes, etc., and how they operate differently from something like a Pandas dataframe. With time, I'm sure these types of things become more familiar, but for those running algorithms/machine learning techniques on smaller datasets, I can definitely see why some data scientists tend to work in a non-distributed system to save on time, confusion (and sometimes efficiency)."],"metadata":{}}],"metadata":{"name":"DATA 612 - Project #5","notebookId":3056149489528635},"nbformat":4,"nbformat_minor":0}
